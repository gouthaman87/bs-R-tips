---
title: "R + Python via reticulate"
description:
  Taking the `radix` R package or a test spin with `Sckit Learn`!
author:
  - name: Gouthaman Tharmathasan
date: "`r Sys.Date()`"
output: radix::radix_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

# Objectives
1. illustrate benefit of using `both Python & R, reticulate package`
2. leverage R for `sleek web-based reporting, new radix package`
3. leverage python for `machine learning algorithms and pipeline, sklearn library`
4. Leverage R for `excellent visualaization capabilities, ggplot2 & plotly`

# R Code

Take advantage of he new radix R package for sleek scientific reporting

### Setup reticulate
```{r, echo=TRUE}
library(reticulate)
conda_list()
use_condaenv("anaconda3")
```

# Switching to Python

### Wine quality tutorial
https://elitedatascience.com/python-machine-learning-tutorial-scikit-learn

### Setup Python modules

```{python, echo=TRUE}
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.datasets import load_wine
```

### import wine data

```{python, echo=TRUE}

wine = load_wine()

data = pd.DataFrame(data = np.c_[wine["data"], wine["target"]],
                    columns= wine["feature_names"] + ["target"])

```


### investigate data
```{r, echo=TRUE}
library(tidyverse)

py$data %>% 
  as.tibble() %>% 
  glimpse()
```

### Scikit Learn-ing Time

Setup data into X (features) & Y (target) variables

```{python, echo=TRUE}
y = data.target
X = data.drop("target", axis=1)
```

Split features into training and testing sets

```{python, echo=TRUE}
X_train, X_test, y_train, y_test = train_test_split(
  X, y,
  test_size = 0.2,
  random_state = 1987,
  stratify = y
)
```

Preprocess by scaling X_train

```{python, echo=TRUE}
scaler = preprocessing.StandardScaler().fit(X_train)
```

Apply transformation to X_test

```{python, echo=TRUE}
X_test_scaled = scaler.transform(X_test)
```

Setup ML Pipeline

```{python, echo=TRUE}
pipeline = make_pipeline(
  preprocessing.StandardScaler(),
  RandomForestRegressor(n_estimators = 100)
)
```

Setup Grid Search

```{python, echo=TRUE}
hyperparameters = {
  "randomforestregressor__max_features" : ["auto", "sqrt", "log2"],
  "randomforestregressor__max_depth" : [None, 5, 3, 1]
}
```

Apply grid search with CV.

```{python, echo=TRUE}
clf = GridSearchCV(pipeline, hyperparameters, cv = 10)
clf.fit(X_train, y_train)
```

Get best parameters

```{python}
print(clf.best_params_)
```

### Make wine predictions & get error metrics

```{python}
y_pred = clf.predict(X_test)

print(r2_score(y_test, y_pred))
```

```{python, echo=TRUE}
print(mean_squared_error(y_test, y_pred))
```

# Back to R Code

### Visulaize the model with ggplot2 & plotly

Plot each predictions & actual data pair from the test set arranged by quality level to visualize how our model  performing

```{r, echo=TRUE}
library(tidyverse)
library(tidyquant)
library(plotly)

results_tbl <- tibble(y_pred = py$y_pred, y_test = py$y_test) %>% 
  rowid_to_column() %>% 
  arrange(y_test) %>% 
  mutate(rowid = as_factor(as.character(rowid))) %>% 
  rowid_to_column("sorted_rowid") %>% 
  pivot_longer(cols = -contains("rowid"), values_to = "value", names_to = "key")

(results_tbl %>% 
    ggplot(aes(rowid, value, color = key)) +
    geom_point(size = 0.5) +
    geom_smooth(aes(sorted_rowid, value)) +
    theme_tq() +
    scale_color_tq() +
    labs(title = "Wine Quality Level Predictions",
         x = "Row ID", y = "Quality Level")) %>% 
  ggplotly()
```

